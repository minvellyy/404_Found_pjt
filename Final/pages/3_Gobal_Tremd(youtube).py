import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from datetime import datetime
import re
from wordcloud import WordCloud
import io
from googleapiclient.discovery import build
import os
import time
import requests 
import openai # OpenAI ì„í¬íŠ¸ ì¶”ê°€

# ========================================
# Streamlit ê¸°ë³¸ ì„¤ì • ë° ê³µí†µ ì—°ê²° í•¨ìˆ˜
# ========================================

# [í†µí•©] DB ì—°ê²° í•¨ìˆ˜ (ì‚¬ìš©í•˜ì§€ ì•Šë”ë¼ë„ ê³µí†µ ëª¨ë“ˆë¡œ ìœ ì§€)
@st.cache_resource
def get_db_connection():
    """Streamlit secrets.tomlì— ì •ì˜ëœ ë‹¨ì¼ MySQL DB ì—°ê²°ì„ ë°˜í™˜"""
    return st.connection("mysql_db", type="sql") 

# [í†µí•©] OpenAI í´ë¼ì´ì–¸íŠ¸ í•¨ìˆ˜ (YouTube ë³´ê³ ì„œ ìƒì„±ì— í•„ìš”)
@st.cache_resource
def get_openai_client():
    """OpenAI API í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”"""
    try:
        # ğŸŸ¢ [ìˆ˜ì •] ì„¹ì…˜ ì—†ì´ ìµœìƒìœ„ ë ˆë²¨ì—ì„œ í‚¤ë¥¼ ì°¾ë„ë¡ ë³€ê²½
        api_key = st.secrets["OPENAI_API_KEY"]
        if not api_key or len(api_key) < 20: 
            st.error("âŒ secrets.tomlì— OPENAI_API_KEYê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
            return None
        return openai.OpenAI(api_key=api_key)
    except KeyError:
        # ğŸŸ¢ [ìˆ˜ì •] ì„¹ì…˜ ì—†ì´ ìµœìƒìœ„ ë ˆë²¨ì—ì„œ í‚¤ë¥¼ ì°¾ë„ë¡ ë³€ê²½
        st.error("âŒ secrets.toml íŒŒì¼ì— 'OPENAI_API_KEY'ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    except Exception as e:
        st.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        return None

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic' 
plt.rcParams['axes.unicode_minus'] = False

SAVE_DIR = "analysis_results"
os.makedirs(SAVE_DIR, exist_ok=True)

# =========================================================================================
# OpenAI Report Generation Functions (API Key ë¡œë“œ ìˆ˜ì •)
# =========================================================================================

def get_openai_api_key():
    """OpenAI API Keyë¥¼ secrets.tomlì—ì„œ ì•ˆì „í•˜ê²Œ ë¡œë“œ"""
    try:
        # ğŸŸ¢ [ìˆ˜ì •] ì„¹ì…˜ ì—†ì´ ìµœìƒìœ„ ë ˆë²¨ì—ì„œ í‚¤ë¥¼ ì°¾ë„ë¡ ë³€ê²½
        return st.secrets["OPENAI_API_KEY"]
    except KeyError:
        # ğŸŸ¢ [ìˆ˜ì •] ì„¹ì…˜ ì—†ì´ ìµœìƒìœ„ ë ˆë²¨ì—ì„œ í‚¤ë¥¼ ì°¾ë„ë¡ ë³€ê²½
        st.error("âŒ secrets.toml íŒŒì¼ì— 'OPENAI_API_KEY'ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    except Exception:
        return None # ì˜¤ë¥˜ ë°œìƒ ì‹œ None ë°˜í™˜

def generate_openai_report(keywords, user_focus_prompt, model_name="gpt-4o"):
    """ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì™€ ë¶„ì„ ë°ì´í„°ë¥¼ ì¡°í•©í•˜ì—¬ OpenAI APIë¥¼ ì´ìš©í•œ ì¼ë°˜ ë³´ê³ ì„œ ë¬¸ì¥ ìƒì„± í•¨ìˆ˜ (í•­ëª©ë‹¹ ìµœëŒ€ 5ë¬¸ì¥ ì œí•œ)"""
    
    api_key = get_openai_api_key()
    if not api_key:
        return "Error: OpenAI API Key is missing. Please set the OPENAI_API_KEY in the secrets.toml file."

    system_prompt = (
        "You are a professional YouTube Market Analyst. "
        "Your task is to analyze the provided raw data summary or statistical analysis "
        # ... (ì´í•˜ í”„ë¡¬í”„íŠ¸ ë™ì¼) ...
        "Do not use markdown headers or lists. Just provide the summary text."
    )
    
    full_user_prompt = (
        f"**User Focus:** {user_focus_prompt}\n\n"
        f"**Analysis Data for Context:** {keywords}\n\n"
        "Generate the insightful report summary in English, focusing on the User Focus above."
    )

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
    }

    payload = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": full_user_prompt}
        ],
        "max_tokens": 250, 
        "temperature": 0.3, 
    }

    try:
        response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload, timeout=40)
        response.raise_for_status() 
        result = response.json()
        summary = result['choices'][0]['message']['content'].strip()
        return summary
    # ... (ì´í•˜ ì—ëŸ¬ ì²˜ë¦¬ ë™ì¼) ...
    except Exception as e:
        return f"API Error occurred: {e}. Check response structure."


def generate_executive_report(keywords, user_exec_prompt, model_name="gpt-4o"):
    """ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì™€ ë¶„ì„ ë°ì´í„°ë¥¼ ì¡°í•©í•˜ì—¬ OpenAI APIë¥¼ ì´ìš©í•œ ì„ì›ì§„ ë³´ê³ ì„œ ìƒì„± (êµ­ë¬¸+ì˜ë¬¸ ë¶„ë¦¬)"""
    
    api_key = get_openai_api_key()
    if not api_key:
        return "Error: OpenAI API Key is missing.", "Error: OpenAI API Key is missing."

    # ğŸŸ¢ [ìˆ˜ì •] AIê°€ ì¶œë ¥ í˜•ì‹ì„ ë°˜ë“œì‹œ ì§€í‚¤ë„ë¡ í”„ë¡¬í”„íŠ¸ì— ëª…ì‹œ
    system_prompt = (
        "You are a Senior Executive Market Analyst specializing in YouTube Trends. "
        "Focus on key insights, strategic implications, and high-level trends. "
        "You MUST generate the report in a dual-language format. "
        "You MUST strictly follow this output format, using these exact labels: "
        "\n\n" # ì¤„ë°”ê¿ˆìœ¼ë¡œ ëª…í™•íˆ êµ¬ë¶„
        "English Summary: [Your analysis in English]"
        "\n\n" # ì¤„ë°”ê¿ˆìœ¼ë¡œ ëª…í™•íˆ êµ¬ë¶„
        "Korean Summary: [Your analysis in Korean]"
    )
    
    # ğŸŸ¢ [ìˆ˜ì •] í”„ë¡¬í”„íŠ¸ì—ì„œ ëª¨í˜¸í•œ ì§€ì‹œ(dual-language format) ì‚­ì œ
    full_user_exec_prompt = (
        f"**User Focus for Executive Report:** {user_exec_prompt}\n\n"
        f"**Analysis Data for Context:** {keywords}\n\n"
        "Generate the Executive Report based on the system instructions."
    )

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
    }
    
    payload = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": full_user_exec_prompt} # ğŸŸ¢ ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸
        ],
        "max_tokens": 800, 
        "temperature": 0.3, 
    }

    try:
        response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload, timeout=50)
        response.raise_for_status()
        result = response.json()
        summary = result['choices'][0]['message']['content'].strip()
        
        # ğŸŸ¢ [ë””ë²„ê¹… ì¶”ê°€] íŒŒì‹± ì „ ì›ë³¸ ì‘ë‹µì„ ì¶œë ¥ (í„°ë¯¸ë„ í™•ì¸ìš©)
        print("--- OpenAI API ì›ë³¸ ì‘ë‹µ ---")
        print(summary)
        print("----------------------------")
        
        eng_match = re.search(r'English Summary:\s*(.*?)\s*Korean Summary:', summary, re.DOTALL)
        kor_match = re.search(r'Korean Summary:\s*(.*)', summary, re.DOTALL)

        english_summary = eng_match.group(1).strip() if eng_match else "Parsing Error: English Summary not found."
        korean_summary = kor_match.group(1).strip() if kor_match else "Parsing Error: Korean Summary not found."

        return english_summary, korean_summary

    except Exception as e:
        error_msg = f"API Error occurred: {e}. Check API key or rate limits."
        return error_msg, error_msg
# =========================================================================================
# ë¶„ì„ í´ë˜ìŠ¤ (YouTubeCommentAnalyzer) (ìˆ˜ì • ì—†ìŒ)
# =========================================================================================

class YouTubeCommentAnalyzer:
    """YouTube ëŒ“ê¸€ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, comments_df, videos_df=None):
        self.comments_df = comments_df.copy()
        self.videos_df = videos_df.copy() if videos_df is not None else None
        
        if 'like_count' not in self.comments_df.columns:
            self.comments_df['like_count'] = 0 
        else:
            self.comments_df['like_count'] = pd.to_numeric(
                self.comments_df['like_count'], errors='coerce'
            ).fillna(0).astype(int)

        if 'published_at' in self.comments_df.columns:
            self.comments_df['published_at'] = pd.to_datetime(self.comments_df['published_at'])
    
    # ... (ì´í•˜ preprocess_text, extract_keywords, wordcloud, 
    #      keyword_frequency, sentiment_keywords, time_trend, 
    #      cooccurrence, topic_comparison í•¨ìˆ˜ë“¤ì€ ì›ë³¸ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€) ...
    def preprocess_text(self, text):
        if pd.isna(text):
            return ""
        text = str(text).lower()
        text = re.sub(r'http\S+|www\S+', '', text)
        text = re.sub(r'[^ê°€-í£a-z0-9\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def extract_keywords(self, min_length=2, top_n=50):
        all_text = ' '.join(self.comments_df['text'].apply(self.preprocess_text))
        words = all_text.split()
        words = [w for w in words if len(w) >= min_length]
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                     'of', 'is', 'are', 'was', 'were', 'been', 'be', 'have', 'has', 'had',
                     'ê·¸', 'ì´', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë“¤', 'ë°', 'ë˜í•œ', 'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤',
                     'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ê·¸ëŸ°', 'ì´ëŸ°', 'ì €ëŸ°'}
        words = [w for w in words if w not in stopwords]
        word_freq = Counter(words)
        return word_freq.most_common(top_n)

    def wordcloud(self, width=1200, height=800):
        all_text = ' '.join(self.comments_df['text'].apply(self.preprocess_text))
        try:
            font_path = 'C:/Windows/Fonts/malgun.ttf'
        except:
            font_path = None
        wordcloud = WordCloud(
            font_path=font_path,
            width=width, height=height, background_color='white',
            max_words=100, relative_scaling=0.3, colormap='viridis'
        ).generate(all_text)
        fig, ax = plt.subplots(figsize=(15, 10))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        ax.set_title('ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ', fontsize=20, pad=20)
        plt.tight_layout()
        return fig

    def keyword_frequency(self, top_n=20):
        keywords = self.extract_keywords(top_n=top_n)
        words, counts = zip(*keywords)
        fig, ax = plt.subplots(figsize=(12, 8))
        ax.barh(range(len(words)), counts, color='skyblue')
        ax.set_yticks(range(len(words)))
        ax.set_yticklabels(words)
        ax.set_xlabel('ë¹ˆë„', fontsize=12)
        ax.set_title(f'ìƒìœ„ {top_n}ê°œ í‚¤ì›Œë“œ ë¹ˆë„', fontsize=16, pad=20)
        ax.invert_yaxis()
        plt.tight_layout()
        freq_df = pd.DataFrame(keywords, columns=['Keyword', 'Frequency'])
        return fig, freq_df

    def sentiment_keywords(self):
        positive_words = {
            'ì¢‹ë‹¤', 'ìµœê³ ', 'ëŒ€ë°•', 'ì˜ˆì˜ë‹¤', 'ì´ì˜ë‹¤', 'ë©‹ì§€ë‹¤', 'í›Œë¥­í•˜ë‹¤', 
            'ì™„ë²½', 'ì¢‹ì•„', 'ê°ì‚¬', 'ì‚¬ë‘', 'í–‰ë³µ', 'ì¶”ì²œ', 'êµ¿', 'good', 
            'best', 'love', 'amazing', 'perfect', 'great', 'excellent',
            'ì¢‹ì•„ìš”', 'ì¢‹ë„¤ìš”', 'ë©‹ìˆë‹¤', 'ì•„ë¦„ë‹µë‹¤', 'ìµœê³ ë‹¤', 'ì§±'
        }
        negative_words = {
            'ì‹«ë‹¤', 'ë³„ë¡œ', 'ì•ˆì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤', 'ìµœì•…', 'í˜•í¸ì—†ë‹¤',
            'ì‹«ì–´', 'ì‹¤ë§', 'ë³„ë¡œë„¤', 'ì•„ì‰½ë‹¤', 'bad', 'worst', 'hate',
            'ì‹«ì–´ìš”', 'ë³„ë¡œì˜ˆìš”', 'ê·¸ì €ê·¸ë ‡ë‹¤', 'ì§€ë£¨í•˜ë‹¤'
        }
        
        def calculate_sentiment(text):
            text = self.preprocess_text(text)
            words = text.split()
            pos_count = sum(1 for w in words if w in positive_words)
            neg_count = sum(1 for w in words if w in negative_words)
            return pos_count, neg_count
        
        self.comments_df[['PositiveCount', 'NegativeCount']] = \
            self.comments_df['text'].apply(lambda x: pd.Series(calculate_sentiment(x)))
        
        def classify_sentiment(row):
            if row['PositiveCount'] > row['NegativeCount']: return 'ê¸ì •'
            elif row['PositiveCount'] < row['NegativeCount']: return 'ë¶€ì •'
            else: return 'ì¤‘ë¦½'
        
        self.comments_df['sentiment'] = self.comments_df.apply(classify_sentiment, axis=1)
        sentiment_counts = self.comments_df['sentiment'].value_counts()
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        colors = ['#90EE90', '#FFB6C1', '#D3D3D3']
        order = ['ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½']
        ordered_counts = sentiment_counts.reindex(order, fill_value=0)
        ordered_counts = ordered_counts[ordered_counts > 0]
        ordered_colors = [c for s, c in zip(order, colors) if s in ordered_counts.index]
        
        if ordered_counts.empty:
            axes[0].set_title('ë°ì´í„° ë¶€ì¡±', fontsize=14, pad=20)
            axes[1].set_title('ë°ì´í„° ë¶€ì¡±', fontsize=14, pad=20)
            axes[0].axis('off')
            axes[1].axis('off')
            sentiment_df = self.comments_df[['text', 'sentiment', 'PositiveCount', 'NegativeCount', 'like_count']].rename(
                columns={'sentiment': 'Sentiment', 'text': 'Text', 'like_count': 'LikeCount'}
            )
            return fig, pd.Series(dtype=int), sentiment_df 

        axes[0].pie(ordered_counts.values, labels=ordered_counts.index, 
                    autopct='%1.1f%%', colors=ordered_colors, startangle=90)
        axes[0].set_title('ëŒ“ê¸€ ê°ì„± ë¶„í¬', fontsize=14, pad=20)
        
        axes[1].bar(ordered_counts.index, ordered_counts.values, color=ordered_colors)
        axes[1].set_xlabel('ê°ì„±', fontsize=12)
        axes[1].set_ylabel('ëŒ“ê¸€ ìˆ˜', fontsize=12)
        axes[1].set_title('ê°ì„±ë³„ ëŒ“ê¸€ ìˆ˜', fontsize=14, pad=20)
        
        plt.tight_layout()
        sentiment_df = self.comments_df[['text', 'sentiment', 'PositiveCount', 'NegativeCount', 'like_count']].rename(
            columns={'sentiment': 'Sentiment', 'text': 'Text', 'like_count': 'LikeCount'}
        )
        return fig, ordered_counts, sentiment_df

    def time_trend(self, interval='D'):
        if 'published_at' not in self.comments_df.columns:
            return None, None
        
        time_counts = self.comments_df.set_index('published_at').resample(interval).size()
        time_likes = self.comments_df.set_index('published_at')['like_count'].resample(interval).sum()
        
        fig, axes = plt.subplots(2, 1, figsize=(14, 10))
        
        axes[0].plot(time_counts.index, time_counts.values, marker='o', linewidth=2)
        axes[0].set_xlabel('ë‚ ì§œ', fontsize=12)
        axes[0].set_ylabel('ëŒ“ê¸€ ìˆ˜', fontsize=12)
        axes[0].set_title('ì‹œê°„ëŒ€ë³„ ëŒ“ê¸€ ìˆ˜ ì¶”ì´', fontsize=14, pad=20)
        axes[0].grid(True, alpha=0.3)
        
        axes[1].plot(time_likes.index, time_likes.values, marker='o', 
                     color='coral', linewidth=2)
        axes[1].set_xlabel('ë‚ ì§œ', fontsize=12)
        axes[1].set_ylabel('ì¢‹ì•„ìš” ìˆ˜', fontsize=12)
        axes[1].set_title('ì‹œê°„ëŒ€ë³„ ì¢‹ì•„ìš” ìˆ˜ ì¶”ì´', fontsize=14, pad=20)
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        trend_df = pd.DataFrame({
            'Date': time_counts.index,
            'CommentCount': time_counts.values,
            'LikeCount': time_likes.values
        })
        return fig, trend_df

    def cooccurrence(self, top_n=15):
        top_keywords = [word for word, _ in self.extract_keywords(top_n=top_n)]
        cooc_matrix = pd.DataFrame(0, index=top_keywords, columns=top_keywords)
        
        for text in self.comments_df['text']:
            text = self.preprocess_text(text)
            words = set(text.split())
            
            for word1 in top_keywords:
                if word1 in words:
                    for word2 in top_keywords:
                        if word2 in words:
                            cooc_matrix.loc[word1, word2] += 1
        
        fig, ax = plt.subplots(figsize=(14, 12))
        sns.heatmap(cooc_matrix, annot=True, fmt='d', cmap='YlOrRd', 
                    cbar_kws={'label': 'ë™ì‹œì¶œí˜„ ë¹ˆë„'}, ax=ax)
        ax.set_title(f'ìƒìœ„ {top_n}ê°œ í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë¶„ì„', fontsize=16, pad=20)
        ax.set_xlabel('Keyword2', fontsize=12) 
        ax.set_ylabel('Keyword1', fontsize=12) 
        plt.tight_layout()
        
        cooc_matrix.index.name = 'Keyword1'
        cooc_matrix.columns.name = 'Keyword2'
        
        return fig, cooc_matrix

    def topic_comparison(self):
        if 'video_title' not in self.comments_df.columns:
            return None, None
        
        video_keywords = {}
        
        for video_title in self.comments_df['video_title'].unique()[:10]:
            video_comments = self.comments_df[
                self.comments_df['video_title'] == video_title
            ]['text']
            
            all_text = ' '.join(video_comments.apply(self.preprocess_text))
            words = all_text.split()
            words = [w for w in words if len(w) >= 2]
            
            word_freq = Counter(words)
            top_words = [word for word, _ in word_freq.most_common(5)]
            
            video_keywords[video_title[:30] + '...'] = top_words
        
        comparison_df = pd.DataFrame(video_keywords).T
        comparison_df.columns = [f'Keyword{i+1}' for i in range(comparison_df.shape[1])]
        comparison_df.index.name = 'VideoTitle' 
        
        fig, ax = plt.subplots(figsize=(14, 8))
        ax.axis('tight')
        ax.axis('off')
        
        cell_text = comparison_df.reset_index().values.tolist() 
        col_labels = ['VideoTitle'] + comparison_df.columns.tolist()
        
        table = ax.table(cellText=cell_text,
                         rowLabels=None, 
                         colLabels=col_labels,
                         cellLoc='center',
                         loc='center',
                         bbox=[0, 0, 1, 1])
        
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1, 2)
        
        for i in range(len(col_labels)):
            table[(0, i)].set_facecolor('#4CAF50')
            table[(0, i)].set_text_props(weight='bold', color='white')
        for i in range(len(comparison_df)):
            table[(i+1, 0)].set_facecolor('#E8F5E9') 
            table[(i+1, 0)].set_text_props(weight='bold')
            
        plt.title('ì˜ìƒë³„ ì£¼ìš” í‚¤ì›Œë“œ ë¹„êµ', fontsize=16, pad=20)
        
        return fig, comparison_df


# =========================================================================================
# YouTube ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ (API Key ë¡œë“œ ìˆ˜ì •)
# =========================================================================================

def search_and_collect_data(keyword, max_videos, max_comments_per_video, order):
    """YouTube APIë¥¼ í†µí•œ ë°ì´í„° ìˆ˜ì§‘ (st.secrets ì‚¬ìš©)"""
    
    # [í†µí•©] secrets.tomlì—ì„œ YouTube API í‚¤ ë¡œë“œ
    try:
        # [youtube] ì„¹ì…˜ì˜ YOUTUBE_API_KEY (ëŒ€ë¬¸ì)
        api_key = st.secrets["youtube"]["YOUTUBE_API_KEY"]
    except KeyError:
        raise ConnectionError("YouTube API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. secrets.tomlì˜ [youtube] ì„¹ì…˜ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    if not api_key:
        raise ConnectionError("YouTube API í‚¤ê°€ secrets.tomlì— ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
    youtube = build("youtube", "v3", developerKey=api_key)
    
    # ... (ì´í•˜ ì˜ìƒ ê²€ìƒ‰, ìƒì„¸ ì •ë³´, ëŒ“ê¸€ ìˆ˜ì§‘ ë¡œì§ì€ ì›ë³¸ê³¼ ë™ì¼) ...
    try:
        search_response = youtube.search().list(
            q=keyword,
            part="snippet",
            maxResults=min(max_videos, 50),
            type="video",
            order=order,
            regionCode="KR"
        ).execute()
        video_ids = [item["id"]["videoId"] for item in search_response["items"]]
    except Exception as e:
        raise ConnectionError(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
    
    videos_data = []
    try:
        for i in range(0, len(video_ids), 50):
            batch_ids = video_ids[i:i+50]
            video_response = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(batch_ids)
            ).execute()
            
            for item in video_response["items"]:
                video_info = {
                    "video_id": item["id"],
                    "title": item["snippet"]["title"],
                    "channel": item["snippet"]["channelTitle"],
                    "published_at": item["snippet"]["publishedAt"],
                    "description": item["snippet"]["description"],
                    "view_count": int(item["statistics"].get("viewCount", 0)),
                    "like_count": int(item["statistics"].get("likeCount", 0)),
                    "comment_count": int(item["statistics"].get("commentCount", 0)),
                    "duration": item["contentDetails"]["duration"],
                    "tags": ", ".join(item["snippet"].get("tags", [])),
                    "url": f"https://www.youtube.com/watch?v={item['id']}"
                }
                videos_data.append(video_info)
    except Exception as e:
        raise ConnectionError(f"ì˜ìƒ ì •ë³´ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
    
    videos_df = pd.DataFrame(videos_data)
    
    all_comments = []
    video_info_dict = {}
    for _, row in videos_df.iterrows():
        video_info_dict[row['video_id']] = {
            'title': row['title'],
            'channel': row['channel'],
            'url': row['url']
        }
    
    for idx, video_id in enumerate(video_ids):
        try:
            comments = []
            next_page_token = None
            
            while len(comments) < max_comments_per_video:
                request = youtube.commentThreads().list(
                    part="snippet,replies",
                    videoId=video_id,
                    maxResults=min(100, max_comments_per_video - len(comments)),
                    pageToken=next_page_token,
                    textFormat="plainText",
                    order="relevance"
                )
                response = request.execute()
                
                for item in response["items"]:
                    top_comment = item["snippet"]["topLevelComment"]["snippet"]
                    
                    comment_info = {
                        "comment_id": item["snippet"]["topLevelComment"]["id"],
                        "video_id": video_id,
                        "author": top_comment["authorDisplayName"],
                        "text": top_comment["textDisplay"],
                        "like_count": top_comment["likeCount"],
                        "published_at": top_comment["publishedAt"],
                        "reply_count": item["snippet"]["totalReplyCount"]
                    }
                    
                    if video_id in video_info_dict:
                        comment_info['video_title'] = video_info_dict[video_id]['title']
                        comment_info['video_channel'] = video_info_dict[video_id]['channel']
                        comment_info['video_url'] = video_info_dict[video_id]['url']
                    
                    comments.append(comment_info)
                
                next_page_token = response.get("nextPageToken")
                if not next_page_token:
                    break
                
                time.sleep(0.5)
            
            all_comments.extend(comments)
        
        except Exception as e:
            if "commentsDisabled" not in str(e):
                pass
        
        time.sleep(1)
    
    comments_df = pd.DataFrame(all_comments)
    
    return videos_df, comments_df


# =========================================================================================
# ìºì‹± ì ìš© í•¨ìˆ˜: ë°ì´í„° ìˆ˜ì§‘ ë° 1íšŒ íŒŒì¼ ì €ì¥ (ìˆ˜ì • ì—†ìŒ)
# =========================================================================================

# ConnectionErrorë¥¼ Streamlit ìœ„ì ¯ ì˜¤ë¥˜ ì—†ì´ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì»¤ìŠ¤í…€ ì˜ˆì™¸ í´ë˜ìŠ¤ ì •ì˜
class ConnectionError(Exception):
    pass

@st.cache_data(show_spinner="YouTube ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ë° ìºì‹± ì¤‘ì…ë‹ˆë‹¤. (ìµœì´ˆ 1íšŒ ì‹¤í–‰)")
def get_and_cache_youtube_data(keyword, max_videos, max_comments_per_video, order):
    """YouTube APIë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ìºì‹œí•©ë‹ˆë‹¤. ì¸ìê°€ ë°”ë€Œì§€ ì•ŠëŠ” í•œ ì¬ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."""
    
    try:
        videos_df, comments_df = search_and_collect_data(keyword, max_videos, max_comments_per_video, order)
    except ConnectionError as e:
        raise ConnectionError(f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘ API ì˜¤ë¥˜ ë°œìƒ: {e}")
    except Exception as e:
        raise ConnectionError(f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}")

    if videos_df is not None and not videos_df.empty:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        videos_file_name = f"youtube_videos_raw_{timestamp}.csv"
        videos_file_path = os.path.join(SAVE_DIR, videos_file_name)
        videos_df.to_csv(videos_file_path, index=False, encoding='utf-8-sig')

        if comments_df is not None and not comments_df.empty:
            comments_file_name = f"youtube_comments_raw_{timestamp}.csv"
            comments_file_path = os.path.join(SAVE_DIR, comments_file_name)
            comments_df.to_csv(comments_file_path, index=False, encoding='utf-8-sig')
    
    return videos_df, comments_df


# ========================================
# Streamlit ë©”ì¸ ì•± (í˜ì´ì§€ ë¡œì§) (API Key ë¡œë“œ ìˆ˜ì •)
# ========================================

def main():
    st.title("ğŸ¥ Gobal Trend(youtube)")
    st.markdown("---")
    
    # [í†µí•©] API Key ë¡œë“œ ë¡œì§ ë³€ê²½ (st.secrets ì°¸ì¡°)
    # ğŸŸ¢ ìˆ˜ì •ëœ get_openai_api_key í•¨ìˆ˜ë¥¼ í˜¸ì¶œ (ì—ëŸ¬ ë©”ì‹œì§€ í‘œì‹œìš©)
    OPENAI_API_KEY_check = get_openai_api_key()
    
    # ì‚¬ì´ë“œë°” - ë°ì´í„° ìˆ˜ì§‘/ì—…ë¡œë“œ
    st.sidebar.header("ğŸ“‚ ë°ì´í„° ì†ŒìŠ¤")
    data_source = st.sidebar.radio(
        "ë°ì´í„° ì…ë ¥ ë°©ì‹ ì„ íƒ",
        ["APIë¡œ ì‹¤ì‹œê°„ ìˆ˜ì§‘", "CSV íŒŒì¼ ì—…ë¡œë“œ"],
        key="youtube_data_source_radio"
    )
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë° ë¡œë“œ
    if 'videos_df' not in st.session_state:
        st.session_state['videos_df'] = None
    if 'comments_df' not in st.session_state:
        st.session_state['comments_df'] = None
    
    videos_df = st.session_state['videos_df']
    comments_df = st.session_state['comments_df']
    
    if data_source == "APIë¡œ ì‹¤ì‹œê°„ ìˆ˜ì§‘":
        st.sidebar.subheader("ğŸ” ê²€ìƒ‰ ì„¤ì •")
        keyword = st.sidebar.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", value="K-beauty", key="youtube_keyword_input")
        max_videos = st.sidebar.slider("ì˜ìƒ ê°œìˆ˜", 1, 50, 10, key="youtube_max_videos_slider")
        max_comments = st.sidebar.slider("ì˜ìƒë‹¹ ëŒ“ê¸€ ìˆ˜", 10, 200, 50, key="youtube_max_comments_slider")
        order = st.sidebar.selectbox(
            "ì •ë ¬ ë°©ì‹",
            ["relevance", "date", "viewCount"],
            format_func=lambda x: {"relevance": "ê´€ë ¨ì„±ìˆœ", "date": "ìµœì‹ ìˆœ", "viewCount": "ì¡°íšŒìˆ˜ìˆœ"}[x],
            key="youtube_order_select"
        )
        
        if st.sidebar.button("ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘", key="youtube_start_collection_button"):
            
            try:
                # ğŸ”´ ìºì‹± í•¨ìˆ˜ í˜¸ì¶œ
                videos_df_new, comments_df_new = get_and_cache_youtube_data(
                    keyword, max_videos, max_comments, order
                )
            except ConnectionError as e:
                st.error(f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                return
            except Exception as e:
                st.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                return

            
            if videos_df_new is not None and comments_df_new is not None and not comments_df_new.empty:
                st.success(f"âœ… ì˜ìƒ {len(videos_df_new)}ê°œ, ëŒ“ê¸€ {len(comments_df_new)}ê°œ ìˆ˜ì§‘/ë¡œë“œ ì™„ë£Œ! (íŒŒì¼ ì €ì¥ ì™„ë£Œ)")
                
                # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì— ì €ì¥ (UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ì¬í• ë‹¹)
                st.session_state['videos_df'] = videos_df_new
                st.session_state['comments_df'] = comments_df_new
                st.rerun() # ë°ì´í„° ìˆ˜ì§‘ í›„ ì•±ì„ ì¬ì‹¤í–‰í•˜ì—¬ UI ì—…ë°ì´íŠ¸
            elif comments_df_new is not None and comments_df_new.empty:
                st.warning("ìˆ˜ì§‘ëœ ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ì¡°ê±´ì´ë‚˜ API ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    
    else:  # CSV íŒŒì¼ ì—…ë¡œë“œ
        st.sidebar.subheader("ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ")
        comments_file = st.sidebar.file_uploader("ëŒ“ê¸€ CSV íŒŒì¼", type=['csv'], key="comments_upload_youtube")
        videos_file = st.sidebar.file_uploader("ì˜ìƒ CSV íŒŒì¼ (ì„ íƒ)", type=['csv'], key="videos_upload_youtube")
        
        if comments_file:
            comments_df = pd.read_csv(comments_file)
            st.session_state['comments_df'] = comments_df
            st.sidebar.success(f"âœ… ëŒ“ê¸€ {len(comments_df)}ê°œ ë¡œë“œ")
        
        if videos_file:
            videos_df = pd.read_csv(videos_file)
            st.session_state['videos_df'] = videos_df
            st.sidebar.success(f"âœ… ì˜ìƒ {len(videos_df)}ê°œ ë¡œë“œ")
        
        if comments_file or videos_file:
            st.rerun()

    # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì•ˆë‚´ ë©”ì‹œì§€
    if comments_df is None or comments_df.empty:
        st.info("ğŸ‘† ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê±°ë‚˜ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        # OPENAI_API_KEY_checkê°€ None (ì¦‰, ë¡œë“œ ì‹¤íŒ¨)ì¼ ë•Œë§Œ ì—ëŸ¬ ë©”ì‹œì§€ê°€ ë–  ìˆìœ¼ë¯€ë¡œ,
        # ë°ì´í„°ê°€ ì—†ì„ ë•Œ ë³„ë„ë¡œ returnì„ í•˜ë˜, í‚¤ ì—ëŸ¬ëŠ” ê·¸ëŒ€ë¡œ ë‘ë„ë¡ í•¨.
        if OPENAI_API_KEY_check is None:
             pass # í‚¤ ì—ëŸ¬ê°€ ì´ë¯¸ ë–  ìˆìœ¼ë¯€ë¡œ ì¶”ê°€ ë™ì‘ ì—†ìŒ
        return # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì—¬ê¸°ì„œ ì¤‘ë‹¨

    # ==================================================================
    # ğŸ‘‡ [ìˆ˜ì •] 'like_count' ì»¬ëŸ¼ ê²€ì¦ ë° ì „ì²˜ë¦¬ (ì´ ë¶€ë¶„ì„ ì¶”ê°€í•´)
    # CSV ì—…ë¡œë“œ ì‹œ 'like_count'ê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, Analyzerì™€ ë™ì¼í•œ ë¡œì§ì„ ì„ ì œì ìœ¼ë¡œ ì ìš©
    if 'like_count' not in comments_df.columns:
        comments_df['like_count'] = 0
    else:
        # 'like_count'ê°€ ìˆì–´ë„ API (ìˆ«ì)ì™€ CSV (ë¬¸ìì—´) íƒ€ì…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°•ì œ ë³€í™˜
        comments_df['like_count'] = pd.to_numeric(
            comments_df['like_count'], errors='coerce'
        ).fillna(0).astype(int)
    
    # ì „ì²˜ë¦¬ëœ DataFrameì„ ì„¸ì…˜ ìƒíƒœì— ë‹¤ì‹œ ì €ì¥
    st.session_state['comments_df'] = comments_df
    # ==================================================================
    
    # ê¸°ë³¸ í†µê³„
    st.header("ğŸ“ˆ ê¸°ë³¸ í†µê³„")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ì´ ëŒ“ê¸€ ìˆ˜", f"{len(comments_df):,}")
    with col2:
        st.metric("í‰ê·  ì¢‹ì•„ìš”", f"{comments_df['like_count'].mean():.1f}") # ğŸ‘ˆ ì´ì œ ì•ˆì „
    with col3:
        st.metric("ì´ ì¢‹ì•„ìš”", f"{comments_df['like_count'].sum():,}") # ğŸ‘ˆ ì—¬ê¸°ë„ ì•ˆì „
    with col4:
        if videos_df is not None:
            st.metric("ë¶„ì„ ì˜ìƒ ìˆ˜", f"{len(videos_df)}")
    
    st.markdown("---")
    
    # íƒ­ìœ¼ë¡œ ë¶„ì„ ëª¨ë“œ êµ¬ë¶„ (ì´ 9ê°œ íƒ­ìœ¼ë¡œ ì¬êµ¬ì„±)
    tabs = st.tabs([
        "â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ",
        "ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„",
        "ğŸ˜ŠğŸ˜¢ ê°ì„± ë¶„ì„",
        "ğŸ“ˆ ì‹œê°„ íŠ¸ë Œë“œ",
        "ğŸ”— ë™ì‹œì¶œí˜„",
        "ğŸ¬ í† í”½ ë¹„êµ",
        "ğŸ“‹ ì›ë³¸ ë°ì´í„°",
        "ğŸ“„ ì¼ë°˜ ë³´ê³ ì„œ", 
        "ğŸ’¼ ì„ì›ì§„ ë³´ê³ ì„œ" 
    ])
    
    analyzer = YouTubeCommentAnalyzer(comments_df, videos_df)
    
    # íƒ­ 1~9 ë¡œì§ (ì›ë³¸ê³¼ ë™ì¼ - key ì¸ìˆ˜, íƒ€ì„ìŠ¤íƒ¬í”„, API í‚¤ ì „ë‹¬ ë¡œì§ ìˆ˜ì •)
    # ... (ì´í•˜ íƒ­[0] ~ íƒ­[6]ì€ Reddit ì½”ë“œì™€ êµ¬ì¡° ë™ì¼, Keyë§Œ ë‹¤ë¦„) ...
    with tabs[0]: 
        st.header("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")
        if st.button("ğŸ” ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±", key="youtube_btn_wordcloud"):
            with st.spinner("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘..."):
                fig = analyzer.wordcloud()
                st.pyplot(fig)
        else:
            st.info("ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.")

    with tabs[1]:
        st.header("ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„")
        top_n = st.slider("í‘œì‹œí•  í‚¤ì›Œë“œ ê°œìˆ˜", 10, 50, 20, key="youtube_keyword_top_n_slider")
        
        if st.button("ğŸ” í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„", key="youtube_btn_keyword"):
            with st.spinner("í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ì¤‘..."):
                fig, freq_df = analyzer.keyword_frequency(top_n=top_n)
                st.pyplot(fig)
                st.session_state['freq_df'] = freq_df 
                
                st.subheader("ğŸ“‹ í‚¤ì›Œë“œ ë°ì´í„° (English Column)")
                st.dataframe(freq_df, use_container_width=True)
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                csv_file_name = f"youtube_keyword_frequency_{timestamp}.csv"
                freq_df.to_csv(os.path.join(SAVE_DIR, csv_file_name), index=False, encoding='utf-8-sig')
                st.success(f"ë¶„ì„ ê²°ê³¼ê°€ ì„œë²„ í´ë” `{SAVE_DIR}`ì— **{csv_file_name}**ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                csv = freq_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button("ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ", csv, csv_file_name, "text/csv", key='youtube_download-keyword-csv')
        else:
            if 'freq_df' in st.session_state:
                st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (í‚¤ì›Œë“œ ë°ì´í„° - English Column)")
                st.dataframe(st.session_state['freq_df'], use_container_width=True)
            else:
                st.info("ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ í‚¤ì›Œë“œ ë¹ˆë„ë¥¼ ë¶„ì„í•˜ì„¸ìš”.")
    
    with tabs[2]:
        st.header("ğŸ˜ŠğŸ˜¢ ê°ì„± ë¶„ì„")
        
        if st.button("ğŸ” ê°ì„± ë¶„ì„ ì‹¤í–‰", key="youtube_btn_sentiment"):
            with st.spinner("ê°ì„± ë¶„ì„ ì¤‘..."):
                fig, sentiment_counts, sentiment_df = analyzer.sentiment_keywords()
                st.pyplot(fig)
                st.session_state['sentiment_df'] = sentiment_df 
                
                col1, col2, col3 = st.columns(3)
                # sentiment_countsê°€ ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ metric í‘œì‹œ
                if not sentiment_counts.empty:
                    for idx, (sentiment, count) in enumerate(sentiment_counts.items()):
                        # sentiment_countsê°€ 3ê°œ ë¯¸ë§Œì¼ ê²½ìš° ëŒ€ë¹„
                        if idx < len([col1, col2, col3]):
                            with [col1, col2, col3][idx]:
                                st.metric(sentiment, f"{count:,}ê°œ")
                
                st.subheader("ğŸ“‹ ê°ì„± ë¶„ë¥˜ ë°ì´í„° (English Column)")
                st.dataframe(sentiment_df.head(100), use_container_width=True)
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                csv_file_name = f"youtube_sentiment_analysis_{timestamp}.csv"
                sentiment_df.to_csv(os.path.join(SAVE_DIR, csv_file_name), index=False, encoding='utf-8-sig')
                st.success(f"ë¶„ì„ ê²°ê³¼ê°€ ì„œë²„ í´ë” `{SAVE_DIR}`ì— **{csv_file_name}**ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                csv = sentiment_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button("ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ", csv, csv_file_name, "text/csv", key='youtube_download-sentiment-csv')
        else:
            if 'sentiment_df' in st.session_state:
                st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (ê°ì„± ë¶„ë¥˜ ë°ì´í„° - English Column)")
                st.dataframe(st.session_state['sentiment_df'].head(100), use_container_width=True)
            else:
                st.info("ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ê°ì„± ë¶„ì„ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
    
    with tabs[3]:
        st.header("ğŸ“ˆ ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„")
        interval = st.radio("ì‹œê°„ ê°„ê²©", ["D (ì¼)", "W (ì£¼)", "M (ì›”)"], horizontal=True, key="youtube_time_interval_radio")
        interval_code = interval.split()[0]
        
        if st.button("ğŸ” ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„", key="youtube_btn_time"):
            with st.spinner("ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘..."):
                fig, trend_df = analyzer.time_trend(interval=interval_code)
                if fig:
                    st.pyplot(fig)
                    st.session_state['trend_df'] = trend_df 
                    st.subheader("ğŸ“‹ íŠ¸ë Œë“œ ë°ì´í„° (English Column)")
                    st.dataframe(trend_df, use_container_width=True)
                    
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    csv_file_name = f"youtube_time_trend_{timestamp}.csv"
                    trend_df.to_csv(os.path.join(SAVE_DIR, csv_file_name), index=False, encoding='utf-8-sig')
                    st.success(f"ë¶„ì„ ê²°ê³¼ê°€ ì„œë²„ í´ë” `{SAVE_DIR}`ì— **{csv_file_name}**ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    csv = trend_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                    st.download_button("ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ", csv, csv_file_name, "text/csv", key='youtube_download-trend-csv')
                else:
                    st.warning("published_at ì»¬ëŸ¼ì´ ì—†ì–´ ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            if 'trend_df' in st.session_state:
                st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (íŠ¸ë Œë“œ ë°ì´í„° - English Column)")
                st.dataframe(st.session_state['trend_df'], use_container_width=True)
            else:
                st.info("ğŸ‘† ì‹œê°„ ê°„ê²©ì„ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.")
    
    with tabs[4]:
        st.header("ğŸ”— í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë¶„ì„")
        cooc_n = st.slider("ë¶„ì„í•  í‚¤ì›Œë“œ ê°œìˆ˜", 5, 20, 15, key="youtube_cooc_n_slider")
        
        if st.button("ğŸ” ë™ì‹œì¶œí˜„ ë¶„ì„", key="youtube_btn_cooc"):
            with st.spinner("ë™ì‹œì¶œí˜„ ë¶„ì„ ì¤‘..."):
                fig, cooc_matrix = analyzer.cooccurrence(top_n=cooc_n)
                st.pyplot(fig)
                st.session_state['cooc_df'] = cooc_matrix 
                
                st.subheader("ğŸ“‹ ë™ì‹œì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤ (English Index/Column)")
                st.dataframe(cooc_matrix, use_container_width=True)
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                csv_file_name = f"youtube_cooccurrence_matrix_{timestamp}.csv"
                cooc_matrix.to_csv(os.path.join(SAVE_DIR, csv_file_name), encoding='utf-8-sig') 
                st.success(f"ë¶„ì„ ê²°ê³¼ê°€ ì„œë²„ í´ë” `{SAVE_DIR}`ì— **{csv_file_name}**ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                csv = cooc_matrix.to_csv(encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button("ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ", csv, csv_file_name, "text/csv", key='youtube_download-cooc-csv')
        else:
            if 'cooc_df' in st.session_state:
                st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (ë™ì‹œì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤ - English Index/Column)")
                st.dataframe(st.session_state['cooc_df'], use_container_width=True)
            else:
                st.info("ğŸ‘† í‚¤ì›Œë“œ ê°œìˆ˜ë¥¼ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.")
    
    with tabs[5]:
        st.header("ğŸ¬ ì˜ìƒë³„ í† í”½ ë¹„êµ")
        
        if st.button("ğŸ” í† í”½ ë¹„êµ ë¶„ì„", key="youtube_btn_topic"):
            with st.spinner("í† í”½ ë¹„êµ ë¶„ì„ ì¤‘..."):
                fig, comparison_df = analyzer.topic_comparison()
                if fig:
                    st.pyplot(fig)
                    st.session_state['topic_df'] = comparison_df 
                    
                    st.subheader("ğŸ“‹ í† í”½ ë¹„êµ ë°ì´í„° (English Column)")
                    st.dataframe(comparison_df, use_container_width=True)
                    
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    csv_file_name = f"youtube_topic_comparison_{timestamp}.csv"
                    comparison_df.to_csv(os.path.join(SAVE_DIR, csv_file_name), encoding='utf-8-sig') 
                    st.success(f"ë¶„ì„ ê²°ê³¼ê°€ ì„œë²„ í´ë” `{SAVE_DIR}`ì— **{csv_file_name}**ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

                    csv = comparison_df.to_csv(encoding='utf-8-sig').encode('utf-8-sig')
                    st.download_button("ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ", csv, csv_file_name, "text/csv", key='youtube_download-topic-csv')
                else:
                    st.warning("video_title ì»¬ëŸ¼ì´ ì—†ì–´ í† í”½ ë¹„êµ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            if 'topic_df' in st.session_state:
                st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (í† í”½ ë¹„êµ ë°ì´í„° - English Column)")
                st.dataframe(st.session_state['topic_df'], use_container_width=True)
            else:
                st.info("ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ í† í”½ ë¹„êµë¥¼ ë¶„ì„í•˜ì„¸ìš”.")

    with tabs[6]:
        st.header("ğŸ“‹ ì›ë³¸ ë°ì´í„°")
        data_type = st.radio("ë°ì´í„° ìœ í˜• ì„ íƒ", ["ëŒ“ê¸€ ë°ì´í„°", "ì˜ìƒ ë°ì´í„°"], horizontal=True, key="youtube_raw_data_type_radio")
        
        if data_type == "ëŒ“ê¸€ ë°ì´í„°":
            st.subheader("ğŸ’¬ ëŒ“ê¸€ ë°ì´í„°")
            st.dataframe(comments_df, use_container_width=True, height=600)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_file_name = f"youtube_comments_raw_{timestamp}.csv"
            
            csv = comments_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
            st.download_button(
                "ğŸ’¾ ëŒ“ê¸€ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ",
                csv, csv_file_name, "text/csv",
                key='youtube_download-comments-raw'
            )
        
        else:
            if videos_df is not None and not videos_df.empty:
                st.subheader("ğŸ¥ ì˜ìƒ ë°ì´í„°")
                st.dataframe(videos_df, use_container_width=True, height=600)
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                csv_file_name = f"youtube_videos_raw_{timestamp}.csv"

                csv = videos_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button(
                    "ğŸ’¾ ì˜ìƒ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ",
                    csv, csv_file_name, "text/csv",
                    key='youtube_download-videos-raw'
                )
            else:
                st.warning("ì˜ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    with tabs[7]:
        st.header("ğŸ“„ Market Insight Report Generator (OpenAI API ê¸°ë°˜)")
        st.write("ë¶„ì„ CSV íŒŒì¼ì— í¬í•¨ëœ í•µì‹¬ í‚¤ì›Œë“œì™€ í†µê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ì‚¬ìš©ì ì§€ì • í”„ë¡¬í”„íŠ¸**ì— ë§ì¶˜ ìš”ì•½ ë³´ê³ ì„œë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤.")
        # [ìˆ˜ì •] .env ê²½ê³ ë¥¼ secrets.toml ê²½ê³ ë¡œ ë³€ê²½
        st.warning("âš ï¸ **OpenAI API Key**ê°€ `secrets.toml`ì— ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•˜ë©°, ë³´ê³ ì„œì— í¬í•¨í•  CSV íŒŒì¼ì€ ë¨¼ì € ë¶„ì„ íƒ­ì—ì„œ ì‹¤í–‰í•˜ê³  **`analysis_results` í´ë”ì— ì €ì¥**í•´ì•¼ í•©ë‹ˆë‹¤.")

        user_focus_prompt = st.text_area(
            "âœï¸ ë³´ê³ ì„œì˜ í•µì‹¬ ë¶„ì„ ì£¼ì œ ë° ì§ˆë¬¸ (Focus Prompt)",
            value="Analyze the key positive and negative sentiment drivers in the selected datasets. What strategic recommendations can be derived from the time trend data for content planning?",
            height=150, key="youtube_report_general_prompt"
        )
        
        try: available_files = [f for f in os.listdir(SAVE_DIR) if f.endswith(".csv")]
        except FileNotFoundError: available_files = []

        if not available_files:
            st.warning("ë¶„ì„ ê²°ê³¼ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ìœ„ ë¶„ì„ íƒ­ì—ì„œ ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  CSVë¥¼ `analysis_results` í´ë”ì— ì €ì¥í•˜ì„¸ìš”.")
        else:
            selected_files = st.multiselect("ğŸ“‚ ë³´ê³ ì„œì— í¬í•¨í•  íŒŒì¼ ì„ íƒ", available_files, default=available_files, key="youtube_report_general_files")

            if st.button("ğŸ§  ë³´ê³ ì„œ ìƒì„± ì‹¤í–‰", key="youtube_btn_generate_report"):
                report_sentences = [] 
                
                # [ìˆ˜ì •] YOUTUBE_API_KEY ë³€ìˆ˜ ëŒ€ì‹  get_openai_api_key() ì‚¬ìš©
                api_key_openai = get_openai_api_key() # ğŸŸ¢ ìˆ˜ì •ëœ í•¨ìˆ˜ í˜¸ì¶œ
                if not selected_files or not user_focus_prompt.strip() or not api_key_openai:
                    st.error("ì…ë ¥ê°’ì„ í™•ì¸í•˜ì„¸ìš”. (íŒŒì¼ ì„ íƒ, í”„ë¡¬í”„íŠ¸ ì…ë ¥, API Key í™•ì¸)")
                    return 
                
                raw_comments_df = st.session_state.get('comments_df')
                if raw_comments_df is None or raw_comments_df.empty:
                    st.error("ëŒ“ê¸€ ì›ë³¸ ë°ì´í„°ê°€ ì„¸ì…˜ì— ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ ë˜ëŠ” ë¡œë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                    return 
                
                temp_analyzer = YouTubeCommentAnalyzer(raw_comments_df, videos_df)
                _, _, sentiment_classified_df_full = temp_analyzer.sentiment_keywords() 


                with st.spinner("OpenAI GPT ëª¨ë¸ì´ ë³´ê³ ì„œë¥¼ ìƒì„± ì¤‘..."):
                    for f in selected_files:
                        file_path = os.path.join(SAVE_DIR, f)
                        try:
                            df = pd.read_csv(file_path, encoding="utf-8-sig")
                            keywords = f"File: {f}. "
                            
                            if 'Frequency' in df.columns: 
                                keywords += f"Top keyword is '{df.iloc[0]['Keyword']}' with count {df.iloc[0]['Frequency']}. Total unique keywords: {len(df)}. "
                            elif 'Sentiment' in df.columns: 
                                sentiment_counts = df['Sentiment'].value_counts()
                                pos = sentiment_counts.get('ê¸ì •', 0)
                                neg = sentiment_counts.get('ë¶€ì •', 0)
                                total = len(df)
                                pos_ratio = pos / total * 100 if total > 0 else 0
                                
                                positive_samples = sentiment_classified_df_full[
                                    (sentiment_classified_df_full['Sentiment'] == 'ê¸ì •') & 
                                    (sentiment_classified_df_full['LikeCount'] > 0) 
                                ].sort_values(by='LikeCount', ascending=False)['Text'].head(3).tolist()
                                
                                if positive_samples:
                                    clean_samples = [temp_analyzer.preprocess_text(s) for s in positive_samples]
                                    sample_text = "Sample positive comments (high like count): " + " | ".join(clean_samples)
                                    keywords += sample_text + " "
                                keywords += f"Total comments {total}. Positive comments: {pos} ({pos_ratio:.1f}%). Negative comments: {neg}. The overall sentiment is mostly Positive. "
                            elif 'CommentCount' in df.columns: 
                                df['Date'] = pd.to_datetime(df['Date'])
                                max_comments_date = df.loc[df['CommentCount'].idxmax(), 'Date'].strftime('%Y-%m-%d')
                                max_comments_count = df['CommentCount'].max()
                                keywords += f"Peak comment count {max_comments_count} occurred on {max_comments_date}. Average comments per period is {df['CommentCount'].mean():.1f}. "
                            elif 'Keyword1' in df.columns and 'Keyword2' in df.columns: 
                                keywords += f"Co-occurrence matrix data. Analyzing relationships between {len(df)} keywords. "
                            elif 'Keyword1' in df.columns: 
                                top_video_topic = df.index[0]
                                key_terms = [str(x) for x in df.iloc[0].dropna().tolist()]
                                keywords += f"The top video topic is '{top_video_topic}' with key terms: {', '.join(key_terms)}. "
                            elif 'text' in df.columns and 'like_count' in df.columns:
                                avg_likes = df['like_count'].mean()
                                top_comments = df.sort_values(by='like_count', ascending=False)['text'].head(3).tolist()
                                clean_samples = [temp_analyzer.preprocess_text(s) for s in top_comments]
                                top_comment_text = " | ".join(clean_samples)
                                keywords += f"Raw comment data summary. Total records: {len(df)}. Average likes per comment: {avg_likes:.1f}. Top comments by like count: {top_comment_text}. "
                            else: 
                                keywords += f"Dataset rows: {len(df)}. Columns: {', '.join(df.columns)}. Data statistics available. "
                            
                            # ğŸŸ¢ ìˆ˜ì •ëœ í•¨ìˆ˜ë¥¼ í†µí•´ ì–»ì€ api_key_openai ë³€ìˆ˜ ì „ë‹¬
                            sentence = generate_openai_report(keywords, user_focus_prompt, model_name="gpt-4o") # api_key ì¸ì ì œê±° (í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ í˜¸ì¶œ)
                            report_sentences.append(f"**{f} Insight:** {sentence}") 
                        except Exception as e:
                            st.error(f"íŒŒì¼ {f} ì²˜ë¦¬ ì˜¤ë¥˜: CSV íŒŒì¼ êµ¬ì¡° í™•ì¸ í•„ìš”. {str(e)}")
                            continue

                if report_sentences:
                    summary = "\n\n".join(report_sentences)
                    final_report = f"""
# YouTube Analysis Auto-Generated Report
## Generated At: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
## User Focus Prompt: {user_focus_prompt}
---
{summary}
"""
                    st.subheader("ğŸ“ˆ AI ìë™ ìƒì„± ë³´ê³ ì„œ ì´ˆì•ˆ")
                    st.text_area("ìš”ì•½ ê²°ê³¼", final_report, height=400)
                    st.download_button(
                        "ğŸ’¾ ìš”ì•½ ë³´ê³ ì„œ ì €ì¥",
                        final_report.encode("utf-8-sig"),
                        "Market_Insight_Report.txt",
                        "text/plain"
                    )
                else:
                    st.error("ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨. íŒŒì¼ ì„ íƒ ë° êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    with tabs[8]:
        st.header("ğŸ’¼ ì„ì›ì§„ ë³´ê³ ì„œ (Executive Summary)")
        st.write("í•µì‹¬ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **êµ­ë¬¸ ë° ì˜ë¬¸**ìœ¼ë¡œ ë¶„ë¦¬ëœ, ì„ì›ì§„ ì œì¶œìš©ìœ¼ë¡œ ì í•©í•œ ìš”ì•½ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        st.warning("âš ï¸ ì´ ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•´ì„œëŠ” **OpenAI API Key**ê°€ í•„ìˆ˜ì´ë©°, ë¶„ì„ íƒ­ì—ì„œ CSV íŒŒì¼ì„ **`analysis_results`** í´ë”ì— ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        user_exec_prompt = st.text_area(
            "âœï¸ ì„ì›ì§„ ë³´ê³ ì„œì˜ í•µì‹¬ ë¶„ì„ ì£¼ì œ ë° ì§ˆë¬¸ (Focus Prompt)",
            value="Identify the 3 most critical market insights regarding K-Beauty trends and propose concise strategic actions for brand positioning based on the competitive analysis.",
            height=100, key="youtube_report_exec_prompt"
        )
        
        try: available_files = [f for f in os.listdir(SAVE_DIR) if f.endswith(".csv")]
        except FileNotFoundError: available_files = []

        if not available_files:
            st.warning("ë¶„ì„ ê²°ê³¼ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¶„ì„ íƒ­ì—ì„œ íŒŒì¼ì„ ì €ì¥í•˜ì„¸ìš”.")
        else:
            selected_exec_files = st.multiselect("ğŸ“‚ ë³´ê³ ì„œì— í¬í•¨í•  íŒŒì¼ ì„ íƒ", available_files, default=available_files, key="youtube_report_exec_files")

            if st.button("ğŸ§  êµ­/ì˜ë¬¸ ì„ì›ì§„ ë³´ê³ ì„œ ìƒì„±", key="youtube_btn_generate_exec_report"):
                
                # [ìˆ˜ì •] API Key ë¡œë“œ
                api_key_openai = get_openai_api_key() # ğŸŸ¢ ìˆ˜ì •ëœ í•¨ìˆ˜ í˜¸ì¶œ
                if not selected_exec_files or not user_exec_prompt.strip() or not api_key_openai:
                    st.error("ì…ë ¥ê°’ì„ í™•ì¸í•˜ì„¸ìš”. (íŒŒì¼ ì„ íƒ, í”„ë¡¬í”„íŠ¸ ì…ë ¥, API Key í™•ì¸)")
                    return 

                raw_comments_df = st.session_state.get('comments_df')
                temp_analyzer = YouTubeCommentAnalyzer(raw_comments_df, videos_df)
                full_keywords_for_exec = ""
                
                with st.spinner("OpenAI GPT ëª¨ë¸ì´ êµ­/ì˜ë¬¸ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ë°ì´í„° ì¤€ë¹„ ì¤‘..."):
                    
                    _, _, sentiment_classified_df_full = temp_analyzer.sentiment_keywords() 
                    
                    for f in selected_exec_files:
                        file_path = os.path.join(SAVE_DIR, f)
                        keywords_chunk = f"File: {f}. "
                        try:
                            df = pd.read_csv(file_path, encoding="utf-8-sig")

                            # --- í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§ (ì¼ë°˜ ë³´ê³ ì„œ íƒ­ê³¼ ë™ì¼) ---
                            if 'Frequency' in df.columns: 
                                keywords_chunk += f"Top keyword is '{df.iloc[0]['Keyword']}' with count {df.iloc[0]['Frequency']}. Total unique keywords: {len(df)}. "
                            elif 'Sentiment' in df.columns: 
                                sentiment_counts = df['Sentiment'].value_counts()
                                pos_ratio = sentiment_counts.get('ê¸ì •', 0) / len(df) * 100 if len(df) > 0 else 0
                                keywords_chunk += f"Total comments {len(df)}. Positive ratio: {pos_ratio:.1f}%. "
                            elif 'CommentCount' in df.columns: 
                                max_count = df['CommentCount'].max()
                                keywords_chunk += f"Peak comment count {max_count}. Average count per period is {df['CommentCount'].mean():.1f}. "
                            elif 'Keyword1' in df.columns and 'Keyword2' in df.columns: 
                                keywords_chunk += f"Co-occurrence matrix data. Analyzing relationships between {len(df)} keywords. "
                            elif 'Keyword1' in df.columns: # Topic Comparison
                                keywords_chunk += f"The top video topic is '{df.index[0]}' with key terms: {', '.join([str(x) for x in df.iloc[0].dropna().tolist()])}. "
                            elif 'text' in df.columns and 'like_count' in df.columns:
                                avg_likes = df['like_count'].mean()
                                keywords_chunk += f"Raw comment data summary. Total records: {len(df)}. Average likes per comment: {avg_likes:.1f}. "
                            else: keywords_chunk += f"Dataset rows: {len(df)}. Columns: {', '.join(df.columns)}. "
                            
                            full_keywords_for_exec += keywords_chunk
                        except Exception as e:
                            st.error(f"íŒŒì¼ {f} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                            continue
                            
                with st.spinner("OpenAI GPT ëª¨ë¸ì´ êµ­/ì˜ë¬¸ ë³´ê³ ì„œë¥¼ ìƒì„± ì¤‘..."):
                    # ğŸŸ¢ api_key ì¸ì ì œê±° (í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ í˜¸ì¶œ)
                    english_summary, korean_summary = generate_executive_report(full_keywords_for_exec, user_exec_prompt)
                    
                    korean_final_report = f"""
# ğŸ”´ YouTube ì„ì›ì§„ ë³´ê³ ì„œ (êµ­ë¬¸)
## ì‘ì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
## ë¶„ì„ ì£¼ì œ: {user_exec_prompt}
---
### í•µì‹¬ ìš”ì•½ (Korean Summary)
{korean_summary}

---
### ë¶„ì„ì— ì‚¬ìš©ëœ ë°ì´í„° íŒŒì¼
{', '.join(selected_exec_files)}
"""
                    english_final_report = f"""
# ğŸ”´ YouTube Executive Summary (English)
## Generated At: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
## Focus Prompt: {user_exec_prompt}
---
### Executive Summary
{english_summary}

---
### Data Files Used
{', '.join(selected_exec_files)}
"""
                    
                    st.markdown("---")
                    st.subheader("ğŸ‡°ğŸ‡· êµ­ë¬¸ ë³´ê³ ì„œ ì´ˆì•ˆ")
                    st.text_area("êµ­ë¬¸ ìš”ì•½ ê²°ê³¼", korean_final_report, height=300)
                    
                    st.subheader("ğŸ‡ºğŸ‡¸ ì˜ë¬¸ ë³´ê³ ì„œ ì´ˆì•ˆ")
                    st.text_area("ì˜ë¬¸ ìš”ì•½ ê²°ê³¼", english_final_report, height=300)
                    
                    col_kor, col_eng = st.columns(2)
                    
                    with col_kor:
                        st.download_button(
                            "ğŸ’¾ êµ­ë¬¸ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ (KR.txt)",
                            korean_final_report.encode("utf-8-sig"),
                            "Executive_Report_KR.txt",
                            "text/plain"
                        )
                    with col_eng:
                        st.download_button(
                            "ğŸ’¾ ì˜ë¬¸ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ (EN.txt)",
                            english_final_report.encode("utf-8-sig"),
                            "Executive_Report_EN.txt",
                            "text/plain"
                        )
                    
                    if "Parsing Error" in english_summary:
                        st.error("ë³´ê³ ì„œ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. OpenAI ì¶œë ¥ í˜•ì‹ì„ í™•ì¸í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()